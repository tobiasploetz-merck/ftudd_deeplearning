{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n ftudd  --file requirements_chem.txt -c pytorch -c rdkit -c conda-forge -c rmg -y\n",
    "# This will take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "conda activate ftudd\n",
    "ipython kernel install --name \"ftudd\" --user\n",
    "# after runnning this cell restart the notebook with the ftudd kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grover model\n",
    "\n",
    "Code and pretrained weights are available from here: https://github.com/tencent-ailab/grover. We will use a fork of it where I already fixed a bug in the code base.\n",
    "\n",
    "Implementation of Yu et al., Self-Supervised Graph Transformer on Large-Scale Molecular Data, NeurIPS 2020\n",
    "\n",
    "Grover is an instance of a graph neural network. It is trained in a self-supervised way, i.e. from unlabeled training data, and creates an embedding of a molecule. It can be fine-tuned for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone Grover repository\n",
    "!git clone https://github.com/emdgroup/grover.git ../grover\n",
    "!mkdir ../grover/data/\n",
    "!wget https://ai.tencent.com/ailab/ml/ml-data/grover-models/pretrain/grover_large.tar.gz -O ../grover/data/grover_large.tar.gz\n",
    "!tar -xzf ../grover/data/grover_large.tar.gz -C ../grover/data/\n",
    "import sys\n",
    "sys.path.append('../grover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grover has a command line interface, let us use it to generate an embedding of a sample molecule\n",
    "import pandas as pd\n",
    "smiles = ['CC(=O)O', 'CCCCCC']\n",
    "pd.DataFrame({'SMILES': smiles}).to_csv('test_smiles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "conda activate ftudd\n",
    "python ../grover/scripts/save_features.py --data_path test_smiles.csv --save_path test_features.npz --features_generator rdkit_2d_normalized --restart \n",
    "python ../grover/main.py fingerprint --data_path test_smiles.csv --features_path test_features.npz --checkpoint_path ../grover/data/grover_large.pt --fingerprint_source both --output test_fingerprints.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fingerprints\n",
    "import numpy as np\n",
    "fingerprints = np.load('test_fingerprints.npz')[\"fps\"]\n",
    "print(fingerprints)\n",
    "print(fingerprints.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChemBERTa model\n",
    "\n",
    "ChemBERTa is based on the BERT NLP model and treats SMILES strings as text that can be modeled. Most NLP models are nicely wrapped by the Huggingface transformer library and hence, we can leverage their API. Further details on ChemBERTa can be found in the paper:\n",
    "\n",
    "Chithrananda et al., ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction, arXiv 2020\n",
    "\n",
    "or at Github: https://github.com/seyonechithrananda/bert-loves-chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Chemberta model\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "chemberta_model_name = 'seyonec/ChemBERTa-zinc-base-v1'\n",
    "chemberta_tokenizer = AutoTokenizer.from_pretrained(chemberta_model_name)\n",
    "chemberta_model = AutoModelForMaskedLM.from_pretrained(chemberta_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def embed_smiles(smiles, tokenizer, model, layers):\n",
    "    \"\"\"\n",
    "    Returns the embedding of a SMILES string.\n",
    "    \"\"\"\n",
    "    # Get the tokenized input\n",
    "    tokenized_input = tokenizer(smiles, return_tensors='pt')\n",
    "    # Get the embedding\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_input, output_hidden_states=True)\n",
    "    # Return the embedding\n",
    "    states = torch.stack([output.hidden_states[l] for l in layers]).mean([1,2]).view(-1)\n",
    "    return states.detach().numpy()\n",
    "\n",
    "test_embedding = embed_smiles('CC(=O)O', chemberta_tokenizer, chemberta_model, [-1])\n",
    "print(test_embedding.shape)\n",
    "print(test_embedding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AqSolDB data \n",
    "# Alternatively, you can use a dataset from here: https://github.com/GLambard/Molecules_Dataset_Collection/tree/master/latest\n",
    "import pandas as pd\n",
    "df_aqsol = pd.read_csv('data_chem/curated-solubility-dataset.csv')\n",
    "print(df_aqsol.shape)\n",
    "print(df_aqsol.head(4))\n",
    "smiles = df_aqsol['SMILES'].values\n",
    "targets = df_aqsol['Solubility'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your tasks\n",
    "1. Create embeddings for the molecules in the AqSolDB dataset using both the pretrained ChemBERTa model as well as the Grover model\n",
    "2. Train a suitable scikit-learn model on top of these embeddings to predict the solubility\n",
    "3. Experiment with this setting and summarize your findings\n",
    "\n",
    "### The advanced stuff\n",
    "4. Fine tune Grover and ChemBERTa on the AqSol prediction task \n",
    "5. Experiment and summarize your findings\n",
    "For fine-tuning Grover have a look at their Github page. For fine-tuning a language model from Huggingface, see here: https://huggingface.co/docs/transformers/training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftudd",
   "language": "python",
   "name": "ftudd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
